{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepDecompiler.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rtldr/DeepDecompiler/blob/master/DeepDecompiler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fctHAUUGgR2k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c96d4c98-5a35-4923-f5a8-843fb5b9e35a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7Ne6O6V01Jn",
        "outputId": "057db970-6847-48cf-af99-e3b1377615a6"
      },
      "source": [
        "!pip3 install pyelftools"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyelftools\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/50/3d7729d64bb23393aa4c166af250a6e6f9def40c90bf0e9af3c5ad25b6f7/pyelftools-0.27-py2.py3-none-any.whl (151kB)\n",
            "\r\u001b[K     |██▏                             | 10kB 16.7MB/s eta 0:00:01\r\u001b[K     |████▎                           | 20kB 19.2MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 30kB 11.0MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 40kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 51kB 4.3MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 61kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 71kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 81kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 92kB 5.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 102kB 5.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 112kB 5.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 122kB 5.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 133kB 5.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 143kB 5.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 153kB 5.9MB/s \n",
            "\u001b[?25hInstalling collected packages: pyelftools\n",
            "Successfully installed pyelftools-0.27\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcPTErLK0h-t"
      },
      "source": [
        "# DATA_FOLDER_PATH = 'drive/MyDrive/data'\n",
        "# Upload the tar.gz data files to the VM and tar -xzvf them\n",
        "DATA_V2_FOLDER_PATH = 'data_v2'\n",
        "DATA_V3_FOLDER_PATH = 'data_v3'\n"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTmr76prHd0-"
      },
      "source": [
        "f = open('train_c_1000.csv', 'a')\n",
        "for i in range (1000):\n",
        "  tkn = CTokenizer()\n",
        "  tokens = tkn.tokenize_file(DATA_V2_FOLDER_PATH + \"/c/\" + str(i) + \".c\")\n",
        "  line = str.lower(tkn.tokens_to_string(tokens)) + '\\n'\n",
        "  f.write(line)\n",
        "f.close()"
      ],
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ioeWLhC0iUE",
        "outputId": "36048903-6e77-4c43-82e1-7006cf6a1b36"
      },
      "source": [
        "# Simple asm tokenizer - TODO maybe look into better ways of doing this\n",
        "from elftools.elf.elffile import ELFFile\n",
        "\n",
        "NUM_TOKENS = 2**8 + 2\n",
        "EOF = 256\n",
        "PAD = 257\n",
        "\n",
        "def tokenize_asm_file(filename):\n",
        "    with open(filename, \"rb\") as f:\n",
        "        elf_file = ELFFile(f)\n",
        "        for section in elf_file.iter_sections():\n",
        "            if section.name == '.text':\n",
        "                bytes_list = list(section.data())\n",
        "    # Add EOF\n",
        "    bytes_list.append(EOF)\n",
        "    return bytes_list\n",
        "    \n",
        "if __name__ == \"__main__\":\n",
        "    m = 0\n",
        "    count = 0\n",
        "    for i in range (1024):\n",
        "        tokenized = tokenize_asm_file(DATA_V2_FOLDER_PATH + \"/asm/\" + str(i))\n",
        "        if len(tokenized) < 100:\n",
        "            count += 1\n",
        "            m = max(m, len(tokenized))\n",
        "    print(m)\n",
        "    print(count)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "99\n",
            "756\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcw3fuGg8Pyw",
        "outputId": "d5d386e2-827f-4022-f24f-2f0555f2ae60"
      },
      "source": [
        "# C Tokenizer\n",
        "\n",
        "from pycparser.c_lexer import CLexer\n",
        "\n",
        "# C token space\n",
        "# 0 - 99 tokens generated by CLexer\n",
        "# 100 - 227 printable ASCII characters (100 -> Ascii 32) TODO - switch to byte pair encodings\n",
        "# 228 - END_ID_CONST\n",
        "# 229 - END_LITERAL\n",
        "# 230 - EOF\n",
        "# 231 - SOS \n",
        "EOF = 100\n",
        "SOS = 101\n",
        "PAD = 102\n",
        "NUM_TOKENS = 103\n",
        "# EOF = 230\n",
        "# SOS = 231\n",
        "# NUM_TOKENS = 232\n",
        "\n",
        "class CTokenizer(BaseTokenizer):\n",
        "    def __init__(self):\n",
        "        self.lexer = CLexer(self._on_error, self._on_left_brace, self._on_left_brace, self._type_lookup_func)\n",
        "        self.lexer.build(optimize=True, lextab='pycparser.lextab', outputdir='')\n",
        "        self.semicolon_token = self.lexer.tokens.index('SEMI')\n",
        "\n",
        "    # Lexer Functions\n",
        "    def _on_error(self, msg, line, column):\n",
        "        print(\"Error tokenizing:\", msg)\n",
        "\n",
        "    def _on_left_brace(self):\n",
        "        pass\n",
        "\n",
        "    def _on_right_brace(self):\n",
        "        pass\n",
        "\n",
        "    def _type_lookup_func(self, name):\n",
        "        return False\n",
        "\n",
        "    def tokenize_file(self, filename, include_SOS=False):\n",
        "        with open(filename) as f:\n",
        "            # Skip first line of file\n",
        "            f.readline()\n",
        "            text = f.read()\n",
        "        tokens = []\n",
        "        self.lexer.input(text)\n",
        "        t = self.lexer.token()\n",
        "        while t is not None:\n",
        "            tokens += self._process_lexer_token(t)\n",
        "            t = self.lexer.token()\n",
        "        tokens.append(EOF)\n",
        "\n",
        "        # Strip header info by removing everything before first two semicolons\n",
        "        tokens = tokens[tokens.index(self.semicolon_token) + 1:]\n",
        "        tokens = tokens[tokens.index(self.semicolon_token) + 1:]\n",
        "\n",
        "        return [SOS] + tokens if include_SOS else tokens \n",
        "    \n",
        "    # Required because of BaseTokenizer\n",
        "    def tokenizer(text):\n",
        "      # Assume text is the actual C code\n",
        "      tokens = []\n",
        "      self.lexer.input(text)\n",
        "      t = self.lexer.token()\n",
        "      while t is not None:\n",
        "          tokens += self._process_lexer_token(t)\n",
        "          t = self.lexer.token()\n",
        "      tokens.append(EOF)\n",
        "\n",
        "      # Strip header info by removing everything before first two semicolons\n",
        "      tokens = tokens[tokens.index(self.semicolon_token) + 1:]\n",
        "      tokens = tokens[tokens.index(self.semicolon_token) + 1:]\n",
        "      output = [SOS] + tokens if include_SOS else tokens\n",
        "      return tokens_to_string(output).split()\n",
        "\n",
        "    def add_special_cases(toks):\n",
        "      pass\n",
        "\n",
        "    def _process_lexer_token(self, ltoken):\n",
        "        ltype = ltoken.type\n",
        "        token = [self.lexer.tokens.index(ltype)]\n",
        "\n",
        "        # if \"LITERAL\" in ltype:\n",
        "        #     value = ltoken.value[1:-1]\n",
        "        #     for char in value:\n",
        "        #         token.append(ord(char) + 100 - 32)\n",
        "        #     token.append(229)\n",
        "        # elif (\"CONST\" in ltype and ltype != \"CONST\") or ltype == \"ID\":\n",
        "        #     value = ltoken.value\n",
        "        #     for char in value:\n",
        "        #         token.append(ord(char) + 100 - 32)\n",
        "        #     token.append(228)\n",
        "\n",
        "        return token\n",
        "\n",
        "    def tokens_to_string(self, tokens):\n",
        "        result = \"\"\n",
        "        for token in tokens:\n",
        "            if 0 <= token < 100:\n",
        "                result += \" \" + self.lexer.tokens[token]\n",
        "            elif 100 <= token < 228:\n",
        "                result += chr(token - 100 + 32)\n",
        "            elif token == 228:\n",
        "                result += \"END_ID_CONST\"\n",
        "            elif token == 229:\n",
        "                result += \"END_STR_LITERAL\"\n",
        "            elif token == EOF:\n",
        "                result += \" EOF\"\n",
        "            else:\n",
        "                result += \" UNKNOWN_TOKEN\"\n",
        "\n",
        "        return result[1:]\n",
        "        \n",
        "    def tokens_to_C_string(self, tokens):\n",
        "        C_dict = {\n",
        "            \"LPAREN\" : '(',\n",
        "            \"RPAREN\" : ')',\n",
        "            # Operators\n",
        "            \"PLUS\" : '+',\n",
        "            \"MINUS\" : '-',\n",
        "            \"TIMES\" : '*',\n",
        "            \"DIVIDE\" : '/',\n",
        "            \"MODULO\" : '%',\n",
        "            \"OR\" : '|',\n",
        "            \"AND\" : '&',\n",
        "            \"NOT\" : '~',\n",
        "            \"XOR\" : '^',\n",
        "            \"LSHIFT\" : '<<',\n",
        "            \"RSHIFT\" : '>>',\n",
        "            \"LOR\" : '||',\n",
        "            \"LAND\" : '&&',\n",
        "            \"LNOT\" : '!',\n",
        "            \"LT\" : '<',\n",
        "            \"GT\" : '>',\n",
        "            \"LE\" : '<=',\n",
        "            \"GE\" : '>=',\n",
        "            \"EQ\" : '==',\n",
        "            \"NE\" : '!=', \n",
        "            # Assignment Operators\n",
        "            \"EQUALS\" : '=',\n",
        "            \"TIMESEQUAL\" : '*=',\n",
        "            \"DIVEQUAL\" : '=',\n",
        "            \"MODEQUAL\" : '=',\n",
        "            \"PLUSEQUAL\" : '+=',\n",
        "            \"MINUSEQUAL\" : '-=',\n",
        "            \"LSHIFTEQUAL\" : '<<=',\n",
        "            \"RSHIFTEQUAL\" : '>>=',\n",
        "            \"ANDEQUAL\" : '&=',\n",
        "            \"OREQUAL\" : '|=',\n",
        "            \"XOREQUAL\" : '^=',\n",
        "            # Increment/decrement\n",
        "            \"PLUSPLUS\" : '++',\n",
        "            \"MINUSMINUS\" : '--',\n",
        "            # ->\n",
        "            \"ARROW\" : '->',\n",
        "            # ?\n",
        "            \"TERNARY\" : '?',\n",
        "            # Delimeters\n",
        "            \"LPAREN\" : '(',\n",
        "            \"RPAREN\" : ')',\n",
        "            \"LBRACKET\" : '[',\n",
        "            \"RBRACKET\" : ']',\n",
        "            \"BRACE\" : '{',\n",
        "            \"RBRACE\" : '}',\n",
        "            \"COMMA\" : ',',\n",
        "            \"PERIOD\" : '.',\n",
        "            \"SEMI\" : ';',\n",
        "            \"COLON\" : ':',\n",
        "            \"ELLIPSIS\" : '...',\n",
        "            \n",
        "        }\n",
        "        result = \"\"\n",
        "        for_semi = 0\n",
        "        indent = 0; \n",
        "        tab = \"    \"\n",
        "        for token in tokens:\n",
        "            if 0 <= token < 100:\n",
        "                token_str = self.lexer.tokens[token]\n",
        "                if token_str == \"LBRACE\":\n",
        "                    result += \"\\n\" + indent*tab\n",
        "                    indent += 1\n",
        "                    result += \"{\\n\" + indent*tab\n",
        "                elif token_str == \"RBRACE\":\n",
        "                    indent -= 1\n",
        "                    result += \"\\n\" + indent*tab\n",
        "                    result += \"}\\n\" + indent*tab\n",
        "                elif token_str == \"FOR\":\n",
        "                    result += \"for\"\n",
        "                    for_semi = 2\n",
        "                elif token_str == \"SEMI\":\n",
        "                    result += \";\"\n",
        "                    if for_semi > 0:\n",
        "                        for_semi -= 1\n",
        "                    else: \n",
        "                        result += \"\\n\" + indent*tab\n",
        "                elif token_str in C_dict: \n",
        "                    result += C_dict[token_str]\n",
        "                else: \n",
        "                    result += \" \" + self.lexer.tokens[token].lower()\n",
        "            elif 100 <= token < 228:\n",
        "                 result += chr(token - 100 + 32)\n",
        "            elif token == 228:\n",
        "                result += \"end_id_const\"\n",
        "            elif token == 229:\n",
        "                result += \"end_str_literal\"\n",
        "            elif token == EOF:\n",
        "                result += \" eof\"\n",
        "            else:\n",
        "                result += \" unknown_token\"\n",
        "\n",
        "        return result[1:]\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    tkn = CTokenizer()\n",
        "    tokens = tkn.tokenize_file(DATA_V2_FOLDER_PATH + \"/c/0.c\")\n",
        "    print(tkn.tokens_to_string(tokens))\n",
        "    print(tkn.tokens_to_C_string(tokens))\n"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "STATIC ID ID LPAREN VOID RPAREN LBRACE ID ID EQUALS INT_CONST_HEX SEMI RETURN ID SEMI RBRACE \n",
            "static id id( void)\n",
            "{\n",
            "     id id= int_const_hex;\n",
            "     return id;\n",
            "    \n",
            "}\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdJPBitL-pUy",
        "outputId": "5ef5b369-8d72-48d3-9261-e34fdfe81798"
      },
      "source": [
        "tokenized = tokenize_asm_file(DATA_V2_FOLDER_PATH + \"/asm/\" + '0')\n",
        "print(tokenized)"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[243, 15, 30, 250, 85, 72, 137, 229, 199, 69, 252, 159, 4, 26, 233, 139, 69, 252, 72, 152, 93, 195, 100]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fO9B3jKlECRV"
      },
      "source": [
        "from fastai import *\n",
        "from fastai.text import *"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FW0kAsXlECt5"
      },
      "source": [
        "# # Create C data loader\n",
        "# c_tokenizer = CTokenizer()\n",
        "# train_tokens = []\n",
        "# train_labels = []\n",
        "# TRAIN_NUM = 1000\n",
        "# for i in range(TRAIN_NUM):\n",
        "#   c_tokens_int = c_tokenizer.tokenize_file(DATA_V2_FOLDER_PATH + '/c/' + str(i) + '.c')\n",
        "#   c_tokens_str = [str(j) for j in c_tokens_int]\n",
        "#   train_tokens.append(c_tokens_str)\n",
        "#   train_labels.append(True)\n",
        "\n",
        "# VAL_NUM = 300\n",
        "# val_tokens = []\n",
        "# val_labels = []\n",
        "# for i in range(TRAIN_NUM, TRAIN_NUM + VAL_NUM):\n",
        "#   c_tokens_int = c_tokenizer.tokenize_file(DATA_V2_FOLDER_PATH + '/c/' + str(i) + '.c')\n",
        "#   c_tokens_str = [str(j) for j in c_tokens_int]\n",
        "#   val_tokens.append(c_tokens_str)\n",
        "#   val_labels.append(True)\n",
        "\n",
        "# tdb = TextDataBunch.from_tokens('./', train_tokens, train_labels, val_tokens, val_labels)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Use ASM numbers as tokens for the model\n",
        "# Use C numbers as tokens for the model\n",
        "\n",
        "# Model thinks str(numbers) are actual words.\n",
        "\n",
        "# Model trains on C str(numbers), spits out a C str(number). We convert all these str(numbers) to C code \n",
        "# strings to see what the model is spitting out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwHVyOHecb6F"
      },
      "source": [
        "class CDataset(Dataset):\n",
        "  def __init__(self, start_index, stop_index):\n",
        "    c_tokenizer = CTokenizer()\n",
        "    self.tokens = []\n",
        "    vocab_len_upper_bound = 0\n",
        "    for i in range(start_index, stop_index):\n",
        "      tokens.append(c_tokenizer.tokenize_file(DATA_V2_FOLDER_PATH + '/c/' + str(i) + '.c'))\n",
        "      c_tokens_str = [str(j) for j in c_tokens_int]\n",
        "      self.tokens.append(c_tokens_str)\n",
        "      vocab_len_upper_bound += len(c_tokens_str)\n",
        "    self.vocab = Vocab.create(self.tokens, vocab_len_upper_bound, 1)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.tokens[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.tokens)\n",
        "\n",
        "train_dataset = CDataset(0, 1000)\n",
        "train_loader = LMDataLoader(dataset=train_dataset)\n",
        "val_dataset = CDataset(1000, 1500)\n",
        "val_loader = LMDataLoader(dataset=val_dataset)\n",
        "\n",
        "data_lm = TextLMDataBunch(train_loader, val_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ak_0kjz7chD7"
      },
      "source": [
        "# Trying to make data_lm through the from_folder method\n",
        "# need tok_func that returns BaseTokenizer\n",
        "\n",
        "def tok_func():\n",
        "  return CTokenizer()\n",
        "\n",
        "tokenizer = Tokenizer(tok_func=tok_func)\n",
        "data_lm = TextLMDataBunch.from_folder(DATA_V3_FOLDER_PATH, tokenizer=tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHGL5egquDlD",
        "outputId": "28ff5048-e98d-478c-c707-2b8e89a2ab02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        }
      },
      "source": [
        "c_tokenizer = CTokenizer()\n",
        "train_tokens = []\n",
        "TRAIN_NUM = 1000\n",
        "for i in range(TRAIN_NUM):\n",
        "  c_tokens_int = c_tokenizer.tokenize_file(DATA_V2_FOLDER_PATH + '/c/' + str(i) + '.c')\n",
        "  c_tokens_str = c_tokenizer.tokens_to_string(c_tokens_int).split()\n",
        "  train_tokens.append(c_tokens_str)\n",
        "train_df = DataFrame(data=train_tokens)\n",
        "\n",
        "VAL_NUM = 300\n",
        "val_tokens = []\n",
        "for i in range(TRAIN_NUM, TRAIN_NUM + VAL_NUM):\n",
        "  c_tokens_int = c_tokenizer.tokenize_file(DATA_V2_FOLDER_PATH + '/c/' + str(i) + '.c')\n",
        "  c_tokens_str = c_tokenizer.tokens_to_string(c_tokens_int).split()\n",
        "  val_tokens.append(c_tokens_str)\n",
        "val_df = DataFrame(data=val_tokens)\n",
        "\n",
        "data_lm = TextLMDataBunch.from_df('.', train_df=train_df, valid_df=val_df, text_cols=765)"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-161-e7aa671cc451>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mval_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mdata_lm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextLMDataBunch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_cols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m765\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/text/data.py\u001b[0m in \u001b[0;36mfrom_df\u001b[0;34m(cls, path, train_df, valid_df, test_df, tokenizer, vocab, classes, text_cols, label_cols, label_delim, chunksize, max_vocab, min_freq, mark_fields, include_bos, include_eos, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m                                    include_bos=include_bos, include_eos=include_eos)\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_listy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_cols\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_cols\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         src = ItemLists(path, TextList.from_df(train_df, path, cols=text_cols, processor=processor),\n\u001b[0m\u001b[1;32m    201\u001b[0m                         TextList.from_df(valid_df, path, cols=text_cols, processor=processor))\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mTextLMDataBunch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_for_lm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/data_block.py\u001b[0m in \u001b[0;36mfrom_df\u001b[0;34m(cls, df, path, cols, processor, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;34m\"Create an `ItemList` in `path` from the inputs in the `cols` of `df`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_names_to_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"You have NaN values in column(s) {cols} of your dataframe, please fix it.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_maybe_squeeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCollection\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minner_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: You have NaN values in column(s) 765 of your dataframe, please fix it."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEzQR6dGdm2H",
        "outputId": "cfdb2468-c5e8-4549-b9ea-691aa0fe088f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "# print(data_lm.train_ds)\n",
        "# print(data_lm.train_ds.vocab)\n",
        "print(train_df)\n",
        "data_lm.show_batch()"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        0      1   2       3       4    ...   761   762   763   764   765\n",
            "0    STATIC     ID  ID  LPAREN    VOID  ...  None  None  None  None  None\n",
            "1    STATIC     ID  ID  LPAREN    VOID  ...  None  None  None  None  None\n",
            "2    STATIC  CONST  ID      ID  LPAREN  ...  None  None  None  None  None\n",
            "3    STATIC     ID  ID  LPAREN    VOID  ...  None  None  None  None  None\n",
            "4    STATIC     ID  ID  LPAREN    VOID  ...  None  None  None  None  None\n",
            "..      ...    ...  ..     ...     ...  ...   ...   ...   ...   ...   ...\n",
            "995  STATIC  CONST  ID      ID  LPAREN  ...  None  None  None  None  None\n",
            "996  STATIC     ID  ID  LPAREN    VOID  ...  None  None  None  None  None\n",
            "997  STATIC     ID  ID  LPAREN    VOID  ...  None  None  None  None  None\n",
            "998  STATIC     ID  ID  LPAREN    VOID  ...  None  None  None  None  None\n",
            "999  STATIC     ID  ID  LPAREN    VOID  ...  None  None  None  None  None\n",
            "\n",
            "[1000 rows x 766 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>idx</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup const xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>xxbos xxup const xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup const xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup const xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>xxup id xxbos xxup const xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup const xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup const xxbos xxup</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup const xxbos xxup const xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos xxup id xxbos</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JV-UixHKS3G8"
      },
      "source": [
        "def tdb_item_to_C_code(item):\n",
        "  int_arr = [int(i) for i in item]\n",
        "  return c_tokenizer.tokens_to_C_string(int_arr)\n",
        "\n",
        "item_idx = 4\n",
        "code = tdb_item_to_C_code(data_lm.train_ds[item_idx])\n",
        "print(code)\n",
        "data_lm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ClLCuPbS3d6"
      },
      "source": [
        "learn_lm = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.3, pretrained=False)"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YngEUPMtXUzQ"
      },
      "source": [
        "learn_lm.predict('38')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DZNsi4arfgF",
        "outputId": "02302336-f1de-44e7-c7f1-f63eb80ac131",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "doc(TextList.from_df)"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h4 id=\"ItemList.from_df\" class=\"doc_header\"><code>from_df</code><a href=\"https://github.com/fastai/fastai/blob/master/fastai/data_block.py#L133\" class=\"source_link\" style=\"float:right\">[source]</a><a class=\"source_link\" data-toggle=\"collapse\" data-target=\"#ItemList-from_df-pytest\" style=\"float:right; padding-right:10px\">[test]</a></h4><blockquote><p><code>from_df</code>(<strong><code>df</code></strong>:<code>DataFrame</code>, <strong><code>path</code></strong>:<code>PathOrStr</code>=<strong><em><code>'.'</code></em></strong>, <strong><code>cols</code></strong>:<code>IntsOrStrs</code>=<strong><em><code>0</code></em></strong>, <strong><code>processor</code></strong>:<code>PreProcessors</code>=<strong><em><code>None</code></em></strong>, <strong>**<code>kwargs</code></strong>) → <code>ItemList</code></p>\n",
              "</blockquote>\n",
              "<div class=\"collapse\" id=\"ItemList-from_df-pytest\"><div class=\"card card-body pytest_card\"><a type=\"button\" data-toggle=\"collapse\" data-target=\"#ItemList-from_df-pytest\" class=\"close\" aria-label=\"Close\"><span aria-hidden=\"true\">&times;</span></a><p>No tests found for <code>from_df</code>. To contribute a test please refer to <a href=\"/dev/test.html\">this guide</a> and <a href=\"https://forums.fast.ai/t/improving-expanding-functional-tests/32929\">this discussion</a>.</p></div></div><p>Create an <a href=\"https://docs.fast.ai/data_block.html#ItemList\"><code>ItemList</code></a> in <code>path</code> from the inputs in the <code>cols</code> of <code>df</code>.</p>\n",
              "<p><a href=\"https://docs.fast.ai/data_block.html#ItemList.from_df\" target=\"_blank\" rel=\"noreferrer noopener\">Show in docs</a></p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dR_yxmRVsGBP",
        "outputId": "1255dd0e-f61a-4e78-ed32-8e5b1a4e313d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "data_lm = TextLMDataBunch.from_csv('.', 'train_c_1000.csv', text_cols=0)"
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/fastai/core.py:302: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return np.array(a, dtype=dtype, **kwargs)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mW36wjFJLa4s",
        "outputId": "03b34695-5c79-4f51-d762-dd69053b8026",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "data_lm.show_batch()"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>idx</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>i d equals int_const_hex semi rbrace else lbrace i d i d equals int_const_dec semi if lparen i d rparen lbrace i d i d equals int_const_hex semi i d equals i d semi i d andequal int_const_hex semi rbrace else lbrace i d i d equals lparen minus int_const_dec rparen semi i d equals i d semi rbrace if lparen i d rparen lbrace i d i d equals</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>d equals i d semi if lparen i d rparen lbrace i d i d equals int_const_hex semi i d i d equals int_const_hex semi i d orequal i d semi i d equals i d semi rbrace else lbrace i d i d equals int_const_hex semi i d i d equals int_const_hex semi i d i d equals int_const_dec semi if lparen i d rparen lbrace i d equals</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>semi return i d semi rbrace xxbos static i d i d lparen void rparen lbrace i d i d equals int_const_dec semi i d i d equals int_const_dec semi for lparen i d equals int_const_oct semi lparen i d lt lparen minus int_const_dec rparen rparen semi i d minusminus rparen lbrace i d i d equals int_const_hex semi for lparen i d equals int_const_oct semi lparen i d gt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>i d equals int_const_dec semi i d i d equals int_const_hex semi i d i d equals lparen minus int_const_dec rparen semi i d i d equals int_const_hex semi i d equals int_const_hex semi i d minusminus semi return i d semi rbrace xxbos static i d i d lparen void rparen lbrace i d i d equals int_const_hex semi return i d semi rbrace xxbos static i d i</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>rparen lbrace const i d i d equals int_const_hex semi i d i d equals int_const_hex semi i d equals i d semi return i d semi rbrace xxbos static i d i d lparen void rparen lbrace i d i d equals lparen minus int_const_dec rparen semi i d i d equals int_const_oct semi i d equals int_const_oct semi for lparen i d equals int_const_oct semi lparen i d</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twruS-MuLdzl",
        "outputId": "9843cb57-8aa7-4eb5-c9e9-22a2655c1ff5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "learn_lm = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.3, pretrained=False)\n",
        "learn_lm.predict('equals')"
      ],
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'equals xxfld'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 196
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNWd4-ACMdAJ",
        "outputId": "1a889c0d-a3ae-4932-bd20-35193cc378a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        }
      },
      "source": [
        "learn_lm.lr_find()"
      ],
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='5' class='' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      83.33% [5/6 00:05<00:01]\n",
              "    </div>\n",
              "    \n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>3.871045</td>\n",
              "      <td>#na#</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.863003</td>\n",
              "      <td>#na#</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.518156</td>\n",
              "      <td>#na#</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.069764</td>\n",
              "      <td>#na#</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.972488</td>\n",
              "      <td>#na#</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='9' class='' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      52.94% [9/17 00:00<00:00 10.5839]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3blBxxTlMo3y",
        "outputId": "d21a2c1d-9f7a-469e-b20f-c2d393d6776f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "learn_lm.recorder.plot()"
      ],
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KqYlgQ7M13C"
      },
      "source": [
        "\n",
        "lr = 1e-007\n"
      ],
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4AUayfoNBbe",
        "outputId": "c8672a7c-a047-4836-82ea-3f5c7e480085",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "learn_lm.fit_one_cycle(1, lr, moms=(0.8,0.7))"
      ],
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>5.197604</td>\n",
              "      <td>4.089928</td>\n",
              "      <td>0.257143</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQfK_7sBNkbU"
      },
      "source": [
        "learn_lm.unfreeze()"
      ],
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YM3GGGANlT_",
        "outputId": "bf2b9b1c-d01d-479d-807e-d94193d82983",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "learn_lm.fit_one_cycle(100, lr, moms=(0.8,0.7))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='63' class='' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      63.00% [63/100 01:14<00:43]\n",
              "    </div>\n",
              "    \n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>3.336360</td>\n",
              "      <td>2.716325</td>\n",
              "      <td>0.257143</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.894108</td>\n",
              "      <td>2.436060</td>\n",
              "      <td>0.257143</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.668029</td>\n",
              "      <td>2.411758</td>\n",
              "      <td>0.257143</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.557141</td>\n",
              "      <td>2.400887</td>\n",
              "      <td>0.257143</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.497025</td>\n",
              "      <td>2.420751</td>\n",
              "      <td>0.257143</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.460772</td>\n",
              "      <td>2.409802</td>\n",
              "      <td>0.257143</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>2.438138</td>\n",
              "      <td>2.440949</td>\n",
              "      <td>0.257143</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.424372</td>\n",
              "      <td>2.422709</td>\n",
              "      <td>0.257143</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>2.420373</td>\n",
              "      <td>2.502876</td>\n",
              "      <td>0.257143</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>2.417042</td>\n",
              "      <td>2.398794</td>\n",
              "      <td>0.257143</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.417737</td>\n",
              "      <td>2.430149</td>\n",
              "      <td>0.257143</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>2.418180</td>\n",
              "      <td>2.530213</td>\n",
              "      <td>0.257143</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>2.501928</td>\n",
              "      <td>2.797754</td>\n",
              "      <td>0.057143</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>2.680245</td>\n",
              "      <td>2.706049</td>\n",
              "      <td>0.257143</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>2.667215</td>\n",
              "      <td>2.522041</td>\n",
              "      <td>0.257143</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>2.603114</td>\n",
              "      <td>2.534518</td>\n",
              "      <td>0.257143</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>2.771383</td>\n",
              "      <td>4.702227</td>\n",
              "      <td>0.257143</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>3.334422</td>\n",
              "      <td>4.231092</td>\n",
              "      <td>0.257143</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>3.786893</td>\n",
              "      <td>3.816732</td>\n",
              "      <td>0.014286</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>3.763145</td>\n",
              "      <td>2.879516</td>\n",
              "      <td>0.257143</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>3.459944</td>\n",
              "      <td>2.739896</td>\n",
              "      <td>0.257143</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>3.175728</td>\n",
              "      <td>2.523627</td>\n",
              "      <td>0.257143</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>2.988235</td>\n",
              "      <td>3.279514</td>\n",
              "      <td>0.257143</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>3.853121</td>\n",
              "      <td>7.724202</td>\n",
              "      <td>0.257143</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>4.679629</td>\n",
              "      <td>4.892144</td>\n",
              "      <td>0.257143</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>5.225664</td>\n",
              "      <td>5.981918</td>\n",
              "      <td>0.114286</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>5.338056</td>\n",
              "      <td>4.113894</td>\n",
              "      <td>0.257143</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>4.888534</td>\n",
              "      <td>2.556105</td>\n",
              "      <td>0.257143</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>4.207366</td>\n",
              "      <td>2.395742</td>\n",
              "      <td>0.257143</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>3.679315</td>\n",
              "      <td>2.428242</td>\n",
              "      <td>0.257143</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>3.307916</td>\n",
              "      <td>2.578886</td>\n",
              "      <td>0.257143</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>3.062093</td>\n",
              "      <td>2.556393</td>\n",
              "      <td>0.257143</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>2.896574</td>\n",
              "      <td>2.466580</td>\n",
              "      <td>0.257143</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>3.066086</td>\n",
              "      <td>7.770992</td>\n",
              "      <td>0.257143</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>4.729976</td>\n",
              "      <td>9.735411</td>\n",
              "      <td>0.257143</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>6.112278</td>\n",
              "      <td>8.095332</td>\n",
              "      <td>0.042857</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>5.944772</td>\n",
              "      <td>4.537584</td>\n",
              "      <td>0.114286</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>5.117606</td>\n",
              "      <td>2.536122</td>\n",
              "      <td>0.257143</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>4.335629</td>\n",
              "      <td>2.422627</td>\n",
              "      <td>0.257143</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>3.770648</td>\n",
              "      <td>2.442572</td>\n",
              "      <td>0.257143</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>3.369275</td>\n",
              "      <td>2.455649</td>\n",
              "      <td>0.257143</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>3.084986</td>\n",
              "      <td>2.444070</td>\n",
              "      <td>0.257143</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>2.882214</td>\n",
              "      <td>2.419592</td>\n",
              "      <td>0.257143</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>2.751693</td>\n",
              "      <td>2.455405</td>\n",
              "      <td>0.257143</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>3.673049</td>\n",
              "      <td>9.882174</td>\n",
              "      <td>0.257143</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>4.930948</td>\n",
              "      <td>6.005881</td>\n",
              "      <td>0.257143</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>4.705195</td>\n",
              "      <td>2.747203</td>\n",
              "      <td>0.257143</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>4.057703</td>\n",
              "      <td>2.468014</td>\n",
              "      <td>0.257143</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>3.572527</td>\n",
              "      <td>2.391788</td>\n",
              "      <td>0.257143</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>3.229175</td>\n",
              "      <td>2.421695</td>\n",
              "      <td>0.257143</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.984777</td>\n",
              "      <td>2.414374</td>\n",
              "      <td>0.257143</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>2.810622</td>\n",
              "      <td>2.387747</td>\n",
              "      <td>0.257143</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>2.687422</td>\n",
              "      <td>2.421018</td>\n",
              "      <td>0.257143</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>2.600922</td>\n",
              "      <td>2.412448</td>\n",
              "      <td>0.257143</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>2.545037</td>\n",
              "      <td>2.412444</td>\n",
              "      <td>0.257143</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>2.503529</td>\n",
              "      <td>2.495739</td>\n",
              "      <td>0.257143</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>2.476097</td>\n",
              "      <td>2.416924</td>\n",
              "      <td>0.257143</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>2.454957</td>\n",
              "      <td>2.447232</td>\n",
              "      <td>0.257143</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>2.440082</td>\n",
              "      <td>2.447686</td>\n",
              "      <td>0.257143</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>2.430459</td>\n",
              "      <td>2.445445</td>\n",
              "      <td>0.257143</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>2.425740</td>\n",
              "      <td>2.412587</td>\n",
              "      <td>0.257143</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>2.436765</td>\n",
              "      <td>2.606943</td>\n",
              "      <td>0.257143</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>2.665221</td>\n",
              "      <td>4.002064</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='1' class='' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      5.88% [1/17 00:00<00:01 2.6957]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-201-45b4e822fc3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn_lm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_one_cycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmoms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/train.py\u001b[0m in \u001b[0;36mfit_one_cycle\u001b[0;34m(learn, cyc_len, max_lr, moms, div_factor, pct_start, final_div, wd, callbacks, tot_epochs, start_epoch)\u001b[0m\n\u001b[1;32m     21\u001b[0m     callbacks.append(OneCycleScheduler(learn, max_lr, moms=moms, div_factor=div_factor, pct_start=pct_start,\n\u001b[1;32m     22\u001b[0m                                        final_div=final_div, tot_epochs=tot_epochs, start_epoch=start_epoch))\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcyc_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m def fit_fc(learn:Learner, tot_epochs:int=1, lr:float=defaults.lr,  moms:Tuple[float,float]=(0.95,0.85), start_pct:float=0.72,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epochs, lr, wd, callbacks)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_fns\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_callback_fns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, learn, callbacks, metrics)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mloss_batch\u001b[0;34m(model, xb, yb, loss_func, opt, cb_handler)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mskip_bwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_backward_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mskip_bwd\u001b[0m\u001b[0;34m:\u001b[0m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_backward_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKvMmrYNNnaw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}