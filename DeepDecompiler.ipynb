{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepDecompiler.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rtldr/DeepDecompiler/blob/master/DeepDecompiler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fctHAUUGgR2k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88ded11b-488e-4af6-b106-a1f5237909f4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7Ne6O6V01Jn",
        "outputId": "d3e51c95-ae6a-4895-ae72-e52f0b6c4237"
      },
      "source": [
        "!pip3 install pyelftools"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyelftools\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/50/3d7729d64bb23393aa4c166af250a6e6f9def40c90bf0e9af3c5ad25b6f7/pyelftools-0.27-py2.py3-none-any.whl (151kB)\n",
            "\r\u001b[K     |██▏                             | 10kB 28.2MB/s eta 0:00:01\r\u001b[K     |████▎                           | 20kB 17.3MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 30kB 10.4MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 40kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 51kB 4.2MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 61kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 71kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 81kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 92kB 5.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 102kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 112kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 122kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 133kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 143kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 153kB 5.8MB/s \n",
            "\u001b[?25hInstalling collected packages: pyelftools\n",
            "Successfully installed pyelftools-0.27\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcPTErLK0h-t"
      },
      "source": [
        "# DATA_FOLDER_PATH = 'drive/MyDrive/data'\n",
        "# Upload the tar.gz data files to the VM and tar -xzvf them\n",
        "DATA_V2_FOLDER_PATH = 'data_v2'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ioeWLhC0iUE",
        "outputId": "eaabfd4d-9513-4b01-ab59-b527c5958eec"
      },
      "source": [
        "# Simple asm tokenizer - TODO maybe look into better ways of doing this\n",
        "from elftools.elf.elffile import ELFFile\n",
        "\n",
        "NUM_TOKENS = 2**8 + 2\n",
        "EOF = 256\n",
        "PAD = 257\n",
        "\n",
        "def tokenize_asm_file(filename):\n",
        "    with open(filename, \"rb\") as f:\n",
        "        elf_file = ELFFile(f)\n",
        "        for section in elf_file.iter_sections():\n",
        "            if section.name == '.text':\n",
        "                bytes_list = list(section.data())\n",
        "    # Add EOF\n",
        "    bytes_list.append(EOF)\n",
        "    return bytes_list\n",
        "    \n",
        "if __name__ == \"__main__\":\n",
        "    m = 0\n",
        "    count = 0\n",
        "    for i in range (1024):\n",
        "        tokenized = tokenize_asm_file(DATA_V2_FOLDER_PATH + \"/asm/\" + str(i))\n",
        "        if len(tokenized) < 100:\n",
        "            count += 1\n",
        "            m = max(m, len(tokenized))\n",
        "    print(m)\n",
        "    print(count)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "99\n",
            "756\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcw3fuGg8Pyw",
        "outputId": "0370450d-1c3c-4cc2-d013-43cb8af9c2b7"
      },
      "source": [
        "# C Tokenizer\n",
        "\n",
        "from pycparser.c_lexer import CLexer\n",
        "\n",
        "# C token space\n",
        "# 0 - 99 tokens generated by CLexer\n",
        "# 100 - 227 printable ASCII characters (100 -> Ascii 32) TODO - switch to byte pair encodings\n",
        "# 228 - END_ID_CONST\n",
        "# 229 - END_LITERAL\n",
        "# 230 - EOF\n",
        "# 231 - SOS \n",
        "EOF = 100\n",
        "SOS = 101\n",
        "PAD = 102\n",
        "NUM_TOKENS = 103\n",
        "# EOF = 230\n",
        "# SOS = 231\n",
        "# NUM_TOKENS = 232\n",
        "\n",
        "class CTokenizer:\n",
        "    def __init__(self):\n",
        "        self.lexer = CLexer(self._on_error, self._on_left_brace, self._on_left_brace, self._type_lookup_func)\n",
        "        self.lexer.build(optimize=True, lextab='pycparser.lextab', outputdir='')\n",
        "        self.semicolon_token = self.lexer.tokens.index('SEMI')\n",
        "\n",
        "    # Lexer Functions\n",
        "    def _on_error(self, msg, line, column):\n",
        "        print(\"Error tokenizing:\", msg)\n",
        "\n",
        "    def _on_left_brace(self):\n",
        "        pass\n",
        "\n",
        "    def _on_right_brace(self):\n",
        "        pass\n",
        "\n",
        "    def _type_lookup_func(self, name):\n",
        "        return False\n",
        "\n",
        "    def tokenize_file(self, filename, include_SOS=False):\n",
        "        with open(filename) as f:\n",
        "            # Skip first line of file\n",
        "            f.readline()\n",
        "            text = f.read()\n",
        "        tokens = []\n",
        "        self.lexer.input(text)\n",
        "        t = self.lexer.token()\n",
        "        while t is not None:\n",
        "            tokens += self._process_lexer_token(t)\n",
        "            t = self.lexer.token()\n",
        "        tokens.append(EOF)\n",
        "\n",
        "        # Strip header info by removing everything before first two semicolons\n",
        "        tokens = tokens[tokens.index(self.semicolon_token) + 1:]\n",
        "        tokens = tokens[tokens.index(self.semicolon_token) + 1:]\n",
        "\n",
        "        return [SOS] + tokens if include_SOS else tokens \n",
        "\n",
        "    def _process_lexer_token(self, ltoken):\n",
        "        ltype = ltoken.type\n",
        "        token = [self.lexer.tokens.index(ltype)]\n",
        "\n",
        "        # if \"LITERAL\" in ltype:\n",
        "        #     value = ltoken.value[1:-1]\n",
        "        #     for char in value:\n",
        "        #         token.append(ord(char) + 100 - 32)\n",
        "        #     token.append(229)\n",
        "        # elif (\"CONST\" in ltype and ltype != \"CONST\") or ltype == \"ID\":\n",
        "        #     value = ltoken.value\n",
        "        #     for char in value:\n",
        "        #         token.append(ord(char) + 100 - 32)\n",
        "        #     token.append(228)\n",
        "\n",
        "        return token\n",
        "\n",
        "    def tokens_to_string(self, tokens):\n",
        "        result = \"\"\n",
        "        for token in tokens:\n",
        "            if 0 <= token < 100:\n",
        "                result += \" \" + self.lexer.tokens[token]\n",
        "            elif 100 <= token < 228:\n",
        "                result += chr(token - 100 + 32)\n",
        "            elif token == 228:\n",
        "                result += \"END_ID_CONST\"\n",
        "            elif token == 229:\n",
        "                result += \"END_STR_LITERAL\"\n",
        "            elif token == EOF:\n",
        "                result += \" EOF\"\n",
        "            else:\n",
        "                result += \" UNKNOWN_TOKEN\"\n",
        "\n",
        "        return result[1:]\n",
        "        \n",
        "    def tokens_to_C_string(self, tokens):\n",
        "        C_dict = {\n",
        "            \"LPAREN\" : '(',\n",
        "            \"RPAREN\" : ')',\n",
        "            # Operators\n",
        "            \"PLUS\" : '+',\n",
        "            \"MINUS\" : '-',\n",
        "            \"TIMES\" : '*',\n",
        "            \"DIVIDE\" : '/',\n",
        "            \"MODULO\" : '%',\n",
        "            \"OR\" : '|',\n",
        "            \"AND\" : '&',\n",
        "            \"NOT\" : '~',\n",
        "            \"XOR\" : '^',\n",
        "            \"LSHIFT\" : '<<',\n",
        "            \"RSHIFT\" : '>>',\n",
        "            \"LOR\" : '||',\n",
        "            \"LAND\" : '&&',\n",
        "            \"LNOT\" : '!',\n",
        "            \"LT\" : '<',\n",
        "            \"GT\" : '>',\n",
        "            \"LE\" : '<=',\n",
        "            \"GE\" : '>=',\n",
        "            \"EQ\" : '==',\n",
        "            \"NE\" : '!=', \n",
        "            # Assignment Operators\n",
        "            \"EQUALS\" : '=',\n",
        "            \"TIMESEQUAL\" : '*=',\n",
        "            \"DIVEQUAL\" : '=',\n",
        "            \"MODEQUAL\" : '=',\n",
        "            \"PLUSEQUAL\" : '+=',\n",
        "            \"MINUSEQUAL\" : '-=',\n",
        "            \"LSHIFTEQUAL\" : '<<=',\n",
        "            \"RSHIFTEQUAL\" : '>>=',\n",
        "            \"ANDEQUAL\" : '&=',\n",
        "            \"OREQUAL\" : '|=',\n",
        "            \"XOREQUAL\" : '^=',\n",
        "            # Increment/decrement\n",
        "            \"PLUSPLUS\" : '++',\n",
        "            \"MINUSMINUS\" : '--',\n",
        "            # ->\n",
        "            \"ARROW\" : '->',\n",
        "            # ?\n",
        "            \"TERNARY\" : '?',\n",
        "            # Delimeters\n",
        "            \"LPAREN\" : '(',\n",
        "            \"RPAREN\" : ')',\n",
        "            \"LBRACKET\" : '[',\n",
        "            \"RBRACKET\" : ']',\n",
        "            \"BRACE\" : '{',\n",
        "            \"RBRACE\" : '}',\n",
        "            \"COMMA\" : ',',\n",
        "            \"PERIOD\" : '.',\n",
        "            \"SEMI\" : ';',\n",
        "            \"COLON\" : ':',\n",
        "            \"ELLIPSIS\" : '...',\n",
        "            \n",
        "        }\n",
        "        result = \"\"\n",
        "        for_semi = 0\n",
        "        indent = 0; \n",
        "        tab = \"    \"\n",
        "        for token in tokens:\n",
        "            if 0 <= token < 100:\n",
        "                token_str = self.lexer.tokens[token]\n",
        "                if token_str == \"LBRACE\":\n",
        "                    result += \"\\n\" + indent*tab\n",
        "                    indent += 1\n",
        "                    result += \"{\\n\" + indent*tab\n",
        "                elif token_str == \"RBRACE\":\n",
        "                    indent -= 1\n",
        "                    result += \"\\n\" + indent*tab\n",
        "                    result += \"}\\n\" + indent*tab\n",
        "                elif token_str == \"FOR\":\n",
        "                    result += \"for\"\n",
        "                    for_semi = 2\n",
        "                elif token_str == \"SEMI\":\n",
        "                    result += \";\"\n",
        "                    if for_semi > 0:\n",
        "                        for_semi -= 1\n",
        "                    else: \n",
        "                        result += \"\\n\" + indent*tab\n",
        "                elif token_str in C_dict: \n",
        "                    result += C_dict[token_str]\n",
        "                else: \n",
        "                    result += \" \" + self.lexer.tokens[token].lower()\n",
        "            elif 100 <= token < 228:\n",
        "                 result += chr(token - 100 + 32)\n",
        "            elif token == 228:\n",
        "                result += \"end_id_const\"\n",
        "            elif token == 229:\n",
        "                result += \"end_str_literal\"\n",
        "            elif token == EOF:\n",
        "                result += \" eof\"\n",
        "            else:\n",
        "                result += \" unknown_token\"\n",
        "\n",
        "        return result[1:]\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    tkn = CTokenizer()\n",
        "    tokens = tkn.tokenize_file(DATA_V2_FOLDER_PATH + \"/c/0.c\")\n",
        "    print(tkn.tokens_to_C_string(tokens))\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "static id id( void)\n",
            "{\n",
            "     id id= int_const_hex;\n",
            "     return id;\n",
            "    \n",
            "}\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KraBcI009AOp",
        "outputId": "b5025a18-7bd2-4bf6-b949-49ef0e7ce3e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        ""
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "static id id( void)\n",
            "{\n",
            "     id id= int_const_hex;\n",
            "     return id;\n",
            "    \n",
            "}\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdJPBitL-pUy",
        "outputId": "1743d251-3005-4050-f98b-408d008678dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tokenized = tokenize_asm_file(DATA_V2_FOLDER_PATH + \"/asm/\" + '0')\n",
        "print(tokenized)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[243, 15, 30, 250, 85, 72, 137, 229, 199, 69, 252, 159, 4, 26, 233, 139, 69, 252, 72, 152, 93, 195, 100]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fO9B3jKlECRV"
      },
      "source": [
        "from fastai import *\n",
        "from fastai.text import *"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FW0kAsXlECt5"
      },
      "source": [
        "# Create C data loader\n",
        "c_tokenizer = CTokenizer()\n",
        "loaders = []\n",
        "for i in range(0):\n",
        "  tokens = c_tokenizer.tokenize_file(DATA_V2_FOLDER_PATH + '/c/' + str(i) + '.c')\n",
        "  dl = DataLoader(tokens)\n",
        "  loaders.append(dl)\n",
        "\n",
        "tdb = TextDataBunch.from_tokens()\n",
        "# Use ASM numbers as tokens for the model\n",
        "# Use C numbers as tokens for the model\n",
        "\n",
        "# Model thinks numbers are actual words.\n",
        "# Model trains on C numbers, spits out a C number. We convert all these numbers to C strings to see what the model is spitting out\n"
      ],
      "execution_count": 29,
      "outputs": []
    }
  ]
}